{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "import Transformers\n",
    "import Estimators\n",
    "import Evaluators\n",
    "import json\n",
    "import utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "                            .appName(\"M5-forecasting\")\\\n",
    "                            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Files\n",
    "\n",
    "sales = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"./M5-forecasting/sales_train_evaluation.csv\")\n",
    "\n",
    "# sales.printSchema()\n",
    "\n",
    "calendar = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"./M5-forecasting/calendar.csv\")\n",
    "\n",
    "# calendar.printSchema()\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "# config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fltr = Transformers.FilterDF(filterCond=\"dept_id == '{}'\".format(config[\"train_on_dept_id\"]))\n",
    "explode_days = Transformers.ExplodingDays(inputCols=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])\n",
    "\n",
    "pipeline = Pipeline(stages=[fltr, explode_days])\n",
    "model = pipeline.fit(sales)\n",
    "filtered_df = model.transform(sales)\n",
    "\n",
    "filtered_df = filtered_df.join(calendar, filtered_df.day == calendar.d)\n",
    "\n",
    "# filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "groupBy = Transformers.GroupByTransformer(groupByCols=[\"store_id\", \"year\", \"month\"], aggExprs={\"sales\": \"sum\"})\n",
    "log_sales = Transformers.LogTransformer(inputCols=[\"sales\"])\n",
    "lag_sales = Transformers.LagFeatures(inputCol=\"sales\", lagVals=config[\"lag_values_to_create\"], partCols=[\"store_id\"], orderCols=[\"year\", \"month\"])\n",
    "fltr_null_lags = Transformers.FilterDF(filterCond=\"lag_sales_{} is not null\".format(config[\"lag_values_to_create\"][-1]))\n",
    "store_indxr = StringIndexer(inputCol=\"store_id\", outputCol=\"store_id_index\")\n",
    "vectorize = VectorAssembler(inputCols=[\"store_id_index\", \"month\", \"year\"] + [\"lag_sales_\" + str(i) for i in range(1, 13)], outputCol=\"features\")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[groupBy, log_sales, lag_sales, fltr_null_lags, store_indxr, vectorize])\n",
    "model = pipeline.fit(filtered_df)\n",
    "transformed_df = model.transform(filtered_df)\n",
    "\n",
    "#transformed_df.show(truncate=7)\n",
    "\n",
    "# Test Train Split\n",
    "trainDF, testDF = utils.df_split(transformed_df, **config[\"train_test_split\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunning hyper-parameters\n",
      "100%|██████████| 40/40 [1:04:38<00:00, 96.96s/trial, best loss: 0.9169676217371655] \n",
      "bestParams:  {'maxBins': 53.0, 'maxDepth': 22.0, 'minInfoGain': 0.0, 'numTrees': 35.0}\n"
     ]
    }
   ],
   "source": [
    "space_pyspark = utils.json_to_space(config[\"hp_space_pyspark_RF\"])\n",
    "\n",
    "bestRFModel = Estimators.RandomForestEstimator(featuresCol=\"features\", labelCol=\"sales\", \n",
    "                                               hyperParamsSpace=space_pyspark, maxIter=40, \n",
    "                                               train_validation_split=config[\"train_validation_split\"])\\\n",
    "                        .fit(trainDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_xgboost = utils.json_to_space(config[\"hp_space_xgboost_RF\"])\n",
    "\n",
    "bestXGBModel = Estimators.XGBoostEstimator(featuresCol=\"features\", labelCol=\"sales\", \n",
    "                                           hyperParamsSpace=space_xgboost, maxIter=10,\n",
    "                                           train_validation_split=config[\"train_validation_split\"])\\\n",
    "                         .fit(trainDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: PySpark Random Forest\n",
      "Loss: 1.078400546495546\n",
      "Evaluating: XGBoost Random Forest\n",
      "Loss: 1.1005501596042446\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the best models\n",
    "\n",
    "bestModels = [{\"model_name\": \"PySpark_Random_Forest\", \"model\": bestRFModel, \"loss\": 0}, \n",
    "              {\"model_name\": \"XGBoost_Random_Forest\", \"model\": bestXGBModel, \"loss\": 0}]\n",
    "\n",
    "evaluator = Evaluators.MAPE(labelCol=\"sales\", predictionCol=\"prediction\")\n",
    "\n",
    "bestModel = {\"model_name\": \"\", \"model\": None, \"loss\": sys.float_info.max, \"predictions\": None}\n",
    "\n",
    "for model_info in bestModels:\n",
    "    print(\"Evaluating: {}\".format(model_info[\"model_name\"]), end='\\t')\n",
    "    preds = model_info[\"model\"].transform(testDF)\n",
    "    model_info[\"loss\"] = evaluator.evaluate(preds)\n",
    "    print(\"Loss: {}\".format(model_info[\"loss\"]))\n",
    "\n",
    "    #Saving the Best Model\n",
    "    if(bestModel[\"loss\"] > model_info[\"loss\"]):\n",
    "        bestModel = model_info.copy()\n",
    "        bestModel[\"predictions\"] = preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = Transformers.AntiLogTransformer(inputCols=[\"sales\", \"prediction\"]).transform(bestModel[\"predictions\"])\n",
    "predictions.select([\"store_id\", \"year\", \"month\", \"sales\", \"prediction\"])\\\n",
    "                 .toPandas()\\\n",
    "                 .to_csv(\"{}_{}_forecasts.csv\".format(bestModel[\"model_name\"], config[\"dept_id\"]), index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9bab137afdcdca85c93cee267b71e6ce2ff5050e816b731a0ee336a18f3c3d6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
