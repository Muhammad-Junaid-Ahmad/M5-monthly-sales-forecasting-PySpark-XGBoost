{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from transformers import *\n",
    "from estimators import *\n",
    "from evaluators import *\n",
    "import json\n",
    "import utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://confiz-5440.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>M5-forecasting</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1957f359820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "                            .appName(\"M5-forecasting\")\\\n",
    "                            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "                            .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")\\\n",
    "                            .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Files\n",
    "\n",
    "sales = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"./M5-forecasting/sales_train_evaluation.csv\")\n",
    "\n",
    "# sales.printSchema()\n",
    "\n",
    "calendar = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"./M5-forecasting/calendar.csv\")\n",
    "\n",
    "# calendar.printSchema()\n",
    "\n",
    "config = json.load(open(\"config.json\"))\n",
    "# config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator definition and decalrations\n",
    "space_pyspark = utils.json_to_space(config[\"hp_space_pyspark_RF\"])\n",
    "rf_est = RandomForestEstimator(featuresCol=\"features\", labelCol=\"sales\", \n",
    "                                               hyperParamsSpace=space_pyspark, maxIter=5, \n",
    "                                               train_validation_split=config[\"train_validation_split\"])\n",
    "\n",
    "space_xgboost = utils.json_to_space(config[\"hp_space_xgboost_RF\"])\n",
    "xgb_est = XGBoostEstimator(featuresCol=\"features\", labelCol=\"sales\", \n",
    "                                           hyperParamsSpace=space_xgboost, maxIter=5,\n",
    "                                           train_validation_split=config[\"train_validation_split\"])\n",
    "\n",
    "space_fbprophet = utils.json_to_space(config[\"hp_space_fbprophet\"])\n",
    "fbp_est  = FBProphetEstimator(featuresCol=\"ds\", labelCol=\"sales\", dataGroupCols=[\"store_id\"],\n",
    "                                                    hyperParamsSpace=space_fbprophet, maxIter=5,\n",
    "                                                    train_validation_split=config[\"train_validation_split\"])\n",
    "\n",
    "estimators_and_models_info = [{\"model_name\": \"RF\", \"estimator\": rf_est}, \n",
    "                              {\"model_name\": \"XGB\", \"estimator\": xgb_est},\n",
    "                              {\"model_name\": \"FBP\", \"estimator\": fbp_est}]\n",
    "# estimators_and_models_info = [{\"model_name\": \"RF\", \"estimator\": rf_est}, {\"model_name\": \"FBP\", \"estimator\": fbp_est}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunning FBProphet hyper-parameters\n",
      "Tunning RF hyper-parameters                          \n",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]\n",
      "Tunning XGB hyper-parameters                                                   \n",
      "  0%|          | 0/5 [02:37<?, ?trial/s, best loss=?]                          \n",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]                          \n",
      "  0%|          | 0/5 [02:37<?, ?trial/s, best loss=?]                          \u001b[A\n",
      "  0%|          | 0/5 [02:38<?, ?trial/s, best loss=?]                          \n",
      " 20%|##        | 1/5 [03:13<12:54, 193.66s/trial, best loss: 2.115797084776122]\n",
      " 20%|##        | 1/5 [07:08<28:35, 428.80s/trial, best loss: 1.0166462183708957]\n",
      " 20%|##        | 1/5 [09:46<12:54, 193.66s/trial, best loss: 2.115797084776122] \u001b[A\n",
      " 20%|##        | 1/5 [09:46<12:54, 193.66s/trial, best loss: 2.115797084776122] \n",
      " 40%|####      | 2/5 [12:46<18:45, 375.16s/trial, best loss: 1.0166462183708957]\n",
      " 20%|##        | 1/5 [15:24<12:54, 193.66s/trial, best loss: 2.115797084776122] \u001b[A\n",
      " 20%|##        | 1/5 [15:24<12:54, 193.66s/trial, best loss: 2.115797084776122] \n",
      "100%|██████████| 5/5 [15:41<00:00, 188.22s/trial, best loss: 1.5131953383427146]\n",
      " 60%|######    | 3/5 [17:53<11:28, 344.30s/trial, best loss: 1.0166462183708957]\n",
      " 20%|##        | 1/5 [20:32<12:54, 193.66s/trial, best loss: 2.115797084776122]\u001b[A\n",
      " 20%|##        | 1/5 [20:32<12:54, 193.66s/trial, best loss: 2.115797084776122]\n",
      " 80%|########  | 4/5 [21:25<04:51, 292.00s/trial, best loss: 1.0166462183708957]\n",
      " 20%|##        | 1/5 [24:03<12:54, 193.66s/trial, best loss: 2.115797084776122]\u001b[A\n",
      " 20%|##        | 1/5 [24:03<12:54, 193.66s/trial, best loss: 2.115797084776122]\n",
      "100%|##########| 5/5 [24:59<00:00, 263.75s/trial, best loss: 1.0166462183708957]\n",
      " 20%|##        | 1/5 [27:37<12:54, 193.66s/trial, best loss: 2.115797084776122]\u001b[A\n",
      " 20%|##        | 1/5 [27:37<12:54, 193.66s/trial, best loss: 2.115797084776122]\n",
      "100%|##########| 5/5 [24:59<00:00, 299.90s/trial, best loss: 1.0166462183708957]\n",
      " 20%|##        | 1/5 [27:37<12:54, 193.66s/trial, best loss: 2.115797084776122]\n",
      "bestParams: \n",
      " 20%|##        | 1/5 [27:37<12:54, 193.66s/trial, best loss: 2.115797084776122]\n",
      "{'colsample_bytree': 0.8, 'max_depth': 78.0, 'n_estimators': 1295.0, 'subsample': 0.9}\n",
      " 20%|##        | 1/5 [27:37<12:54, 193.66s/trial, best loss: 2.115797084776122]\n",
      " 40%|####      | 2/5 [30:24<51:56, 1038.88s/trial, best loss: 1.045974975948118]\n",
      " 60%|######    | 3/5 [30:36<19:00, 570.13s/trial, best loss: 1.045974975948118] \n",
      " 80%|########  | 4/5 [30:47<05:49, 349.49s/trial, best loss: 1.045974975948118]\n",
      "100%|##########| 5/5 [30:59<00:00, 227.70s/trial, best loss: 1.045974975948118]\n",
      "100%|##########| 5/5 [30:59<00:00, 371.90s/trial, best loss: 1.045974975948118]\n",
      "bestParams: \n",
      "{'maxBins': 52.0, 'maxDepth': 22.0, 'minInfoGain': 0.0, 'numTrees': 30.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[store_id: string, year: int, month: int, sales: double, lag_sales_1: double, lag_sales_2: double, lag_sales_3: double, lag_sales_4: double, lag_sales_5: double, lag_sales_6: double, lag_sales_7: double, lag_sales_8: double, lag_sales_9: double, lag_sales_10: double, lag_sales_11: double, lag_sales_12: double, store_id_index: double, features: vector, ds: date, train_test_val_indicator: int, RF_prediction: double, XGB_prediction: double, FBP_prediction: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "fltr = FilterDF(filterCond=\"dept_id == '{}'\".format(config[\"train_on_dept_id\"]))\n",
    "explode_days = ExplodeDays(inputCols=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])\n",
    "join_dfs = DFsJoiner(joinOn={\"df1_col\": \"day\", \"df2_col\": \"d\"}, df2=calendar)\n",
    "groupBy = GroupByTransformer(groupByCols=[\"store_id\", \"year\", \"month\"], aggExprs={\"sales\": \"sum\"})\n",
    "log_sales = LogTransformer(inputCols=[\"sales\"])\n",
    "lag_sales = LagFeatures(inputCol=\"sales\", lagVals=config[\"lag_values_to_create\"], partCols=[\"store_id\"], orderCols=[\"year\", \"month\"])\n",
    "fltr_null_lags = FilterDF(filterCond=\"lag_sales_{} is not null\".format(config[\"lag_values_to_create\"][-1]))\n",
    "store_indxr = StringIndexer(inputCol=\"store_id\", outputCol=\"store_id_index\")\n",
    "vectorize = VectorAssembler(inputCols=[\"store_id_index\", \"month\", \"year\"] + [\"lag_sales_\" + str(i) for i in range(1, 13)], outputCol=\"features\")\n",
    "date_stamp = GenerateDSTransformer(inputCols=[\"year\", \"month\"], outputCol=\"ds\")\n",
    "\n",
    "train_test_val_ind = TrainTestValIndicator(outputCol=\"train_test_val_indicator\", \n",
    "                                           indicatorCond = {\"train_before\": config[\"train_validation_split\"], \n",
    "                                                            \"test_after\": config[\"train_test_split\"] })\n",
    "\n",
    "#This will fit all the given estimators and return a global model containing list of all trained models\n",
    "#its fit method will also update every dict of the \"estimators_and_models_info\" list with key \"model\" containing the fitted model\n",
    "run_estimators = MasterEstimator(estimatorsToFit = estimators_and_models_info) \n",
    "best_model_selection = BestModelSelection(evaluator=MAPE, models=estimators_and_models_info, validationIndicatorCol=\"train_test_val_indicator\", labelCol=\"sales\")\n",
    "\n",
    "\n",
    "df, models, best_model_ind = Pipeline(stages=[fltr, \n",
    "                                            explode_days, \n",
    "                                            join_dfs, \n",
    "                                            groupBy, \n",
    "                                            log_sales, \n",
    "                                            lag_sales, \n",
    "                                            fltr_null_lags, \n",
    "                                            store_indxr, \n",
    "                                            vectorize, \n",
    "                                            date_stamp,\n",
    "                                            train_test_val_ind,\n",
    "                                            run_estimators,\n",
    "                                            best_model_selection])\\\n",
    "                                        .fit(sales)\\\n",
    "                                        .transform(sales)\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving predictions of best model into CSV\n",
    "predCol = models[best_model_ind][\"model_name\"] + \"_prediction\"\n",
    "best_pred_df = df.filter(\"train_test_val_indicator == 2\")\\\n",
    "                 .select([\"store_id\", \"year\", \"month\", \"sales\", predCol])\\\n",
    "                 .orderBy(\"store_id\", \"year\", \"month\")\n",
    "best_pred_df = AntiLogTransformer(inputCols=[\"sales\", predCol]).transform(best_pred_df)\n",
    "best_pred_df.toPandas()\\\n",
    "            .to_csv(\"{}_{}_forecasts.csv\".format(models[best_model_ind][\"model_name\"], \n",
    "                                                 config[\"train_on_dept_id\"]), \n",
    "                    index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9bab137afdcdca85c93cee267b71e6ce2ff5050e816b731a0ee336a18f3c3d6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
